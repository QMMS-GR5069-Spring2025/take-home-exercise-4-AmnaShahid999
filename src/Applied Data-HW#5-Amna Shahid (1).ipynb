{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e2b80030-5b8d-4015-a0a7-019a350aec99",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pip install mlflow==2.11.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d0a6b49-77ff-43cb-a5f3-6a91b7ab774c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import boto3\n",
    "import io\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tempfile\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from pyspark.sql.functions import concat_ws, col\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6eb6c064-e9c6-446c-beea-9844f9994f6a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "s3 = boto3.client('s3')\n",
    "bucket = \"columbia-gr5069-main\"\n",
    "\n",
    "# Define a helper to read S3 CSV\n",
    "def read_s3_csv(key):\n",
    "    obj = s3.get_object(Bucket=bucket, Key=key)\n",
    "    return pd.read_csv(io.BytesIO(obj['Body'].read()))\n",
    "\n",
    "# Load datasets from S3\n",
    "results = read_s3_csv(\"raw/results.csv\") #info on rank, fastest lap time and speed\n",
    "races = read_s3_csv(\"raw/races.csv\") #dates and names of all the races but has a lot of empty data\n",
    "drivers = read_s3_csv(\"raw/drivers.csv\") #drivers names and nationalities\n",
    "lap_times = read_s3_csv(\"raw/lap_times.csv\") #for each race, for each driver, the lap time, the number of laps and the time of the fastest lap\n",
    "pit_stops = read_s3_csv(\"raw/pit_stops.csv\") #time spent at the pitstop in each lap\n",
    "qualifying = read_s3_csv(\"raw/qualifying.csv\") #time taken in each round-the ones who dont finish or are disqualified are /N"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9a4a8e7f-476e-4bd6-a622-2a4cadf89b0a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "want to see what each dataset means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d32280b-3ed9-4ac5-a117-1e9472cc1e60",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display (results)\n",
    "     \n",
    "\n",
    "display (races)\n",
    "     \n",
    "\n",
    "display(drivers)\n",
    "     \n",
    "\n",
    "display(lap_times)\n",
    "\n",
    "     \n",
    "\n",
    "display(pit_stops)\n",
    "     \n",
    "\n",
    "display(qualifying)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "91095de3-f87d-448c-bb36-0ed88dbfc084",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Preparing dataset by combining results with pitstops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "346ddb8b-a331-4146-9a02-39038161c7f1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "%python\n",
    "# Reset the index of the results DataFrame\n",
    "results_reset = results.reset_index()\n",
    "\n",
    "# Join results df with pitstops df with suffixes for overlapping columns\n",
    "pitstop_results_df = pit_stops.join(\n",
    "    results_reset.set_index(['raceId', 'driverId']),\n",
    "    on=['raceId', 'driverId'],\n",
    "    how='inner',\n",
    "    lsuffix='_pitstop',\n",
    "    rsuffix='_result'\n",
    ")\n",
    "\n",
    "display(pitstop_results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e232277-08e9-4865-a0c0-5462a18f7fde",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#get a list of all columns in the dataset\n",
    "df = pitstop_results_df.columns\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1e6230b8-55c2-4fbc-8c63-e303ddaf5672",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "     \n",
    "positionOrder: Final race classification (e.g., 1 = winner).\n",
    "\n",
    "rank: Ranking of the driver's fastest lap in the race (e.g., 1 = fastest lap overall).\n",
    "\n",
    "fastestLap: Lap number where the driver set their fastest lap.\n",
    "\n",
    "Objective: I want to predict position order using pitstop_results_df\n",
    "Feature Selection\n",
    "Using relevant columns from the dataset:\n",
    "Pre-Race Features:\n",
    "grid (starting position), constructorId (team), driverId (driver skill).\n",
    "\n",
    "In-Race Features:\n",
    "laps (completed laps), statusId (DNF flag), fastestLapSpeed, fastestLapTime.\n",
    "\n",
    "Pit Stop Features:\n",
    "stop (number of pit stops), milliseconds_pitstop (total pit time)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b87520f-cd7d-4e97-b111-427b87f3208c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Select features for modeling\n",
    "model_data = pitstop_results_df[['grid','constructorId','raceId','driverId','laps','statusId','fastestLapTime', 'fastestLapSpeed','stop','milliseconds_pitstop','rank','positionOrder']]\n",
    "                            \n",
    "display(model_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "491fa9da-96c9-4239-8d3c-753d78af90cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "I'm only interested in those who finsihed the race, so statusId=1 is the only valid metric for me\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "386966ee-6b1c-4628-9fe4-9ebf1d267f97",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "model_data['is_DNF'] = (model_data['statusId'] != 1).astype(int)  # 1=Finished, 0=DNF\n",
    "display(model_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "808a4f63-8ca2-4497-a360-231e37ed4efb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "#want to check the datatypes and if i have any nans\n",
    "print(model_data.dtypes)\n",
    "print(model_data.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f9e9a962-35ae-49b8-91da-3d42a5219b5e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b95c18d1-4bed-40c6-9c53-9731904a6ff1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "df=model_data\n",
    "# Step 1: Replace '\\N' with NaN in problematic columns\n",
    "cols_with_N = [\n",
    "    'grid','constructorId','raceId','driverId','laps','statusId','fastestLapTime', 'fastestLapSpeed','stop','milliseconds_pitstop','rank','positionOrder'\n",
    "]\n",
    "cols_existing = [col for col in cols_with_N if col in model_data.columns]\n",
    "model_data[cols_existing] = model_data[cols_existing].replace(r'\\N', np.nan)\n",
    "\n",
    "# Step 3: Drop non-numeric columns OR encode them if needed\n",
    "df = model_data.select_dtypes(include=[np.number])  \n",
    "\n",
    "# Step 4: Drop remaining rows with missing data \n",
    "df = df.dropna()\n",
    "\n",
    "#Model Prep\n",
    "\n",
    "# Step 5: Define features and target\n",
    "X = df.drop(columns=[\"positionOrder\"])\n",
    "y = df[\"positionOrder\"]\n",
    "\n",
    "# Step 6: Train-test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a9afcf2-0955-4756-9f24-6b043ec5de67",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "with mlflow.start_run(run_name='RandomForest') as run:\n",
    "  # Create model, train it, and create predictions\n",
    "  rf = RandomForestRegressor(n_estimators=100, max_depth=10)\n",
    "  rfFit= rf.fit(X_train, y_train)\n",
    "  predictionsRF = rf.predict(X_test)\n",
    "\n",
    "  # Log model\n",
    "  mlflow.sklearn.log_model(rf, \"RandomForestRegressor-model\")\n",
    "\n",
    "  # Create metrics\n",
    "  rmse_rf = mean_squared_error(y_test, predictionsRF, squared=False)\n",
    "  mse_rf = mean_squared_error(y_test, predictionsRF)\n",
    "  mae_rf = mean_absolute_error(y_test, predictionsRF)\n",
    "  r2_rf = r2_score(y_test, predictionsRF)\n",
    "\n",
    "  \n",
    "  # Log model and metrics\n",
    "  mlflow.sklearn.log_model(rfFit, \"random_forest_model\")\n",
    "  mlflow.log_metric(\"rmse\", rmse_rf)\n",
    "  mlflow.log_metric(\"r2\", r2_rf)\n",
    "  mlflow.log_metric(\"mae\", mae_rf)\n",
    "  mlflow.log_metric(\"mse\", mse_rf)\n",
    "  mlflow.log_param(\"model_type\", \"RandomForestRegressor\")\n",
    "  mlflow.log_param(\"numTrees\", 100)\n",
    "  mlflow.log_param(\"maxDepth\", 10)\n",
    "  # Saving and logging prediction CSV\n",
    "  predRF_final = pd.DataFrame({\n",
    "    'features': X_test.values.tolist(),\n",
    "    'target': y_test,\n",
    "    'prediction': predictionsRF\n",
    "  })\n",
    "  rf_csv_path = \"/tmp/rf_predictions.csv\"\n",
    "  predRF_final.to_csv(rf_csv_path, index=False)\n",
    "  mlflow.log_artifact(rf_csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae5b379f-c212-4177-82dc-5c58b4ad8a9a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Using a linear regression with ML Flow to predict the target 'positionOrder'.\n",
    "mlflow.end_run() \n",
    "with mlflow.start_run(run_name=\"LinearRegression\"):\n",
    "    lr = LinearRegression()\n",
    "    lrFit= lr.fit(X_train, y_train)\n",
    "    predictionsLR = lr.predict(X_test)\n",
    "    # Log model\n",
    "    mlflow.sklearn.log_model(lr, \"LinearRegression-model\")\n",
    "\n",
    "    # Create metrics\n",
    "    rmse_lr = mean_squared_error(y_test, predictionsLR, squared=False)\n",
    "    mse_lr = mean_squared_error(y_test, predictionsLR)\n",
    "    mae_lr = mean_absolute_error(y_test, predictionsLR)\n",
    "    r2_lr = r2_score(y_test, predictionsLR)\n",
    "\n",
    "    # Log model and metrics\n",
    "    mlflow.sklearn.log_model(lrFit, \"linear_regression_model\")\n",
    "    mlflow.log_metric(\"rmse\", rmse_lr)\n",
    "    mlflow.log_metric(\"r2\", r2_lr)\n",
    "    mlflow.log_metric(\"mae\", mae_lr)\n",
    "    mlflow.log_metric(\"mse\", mse_lr)\n",
    "    mlflow.log_param(\"model_type\", \"LinearRegression\")\n",
    "\n",
    "    #Saving and logging prediction CSV as a second artifact\n",
    "    predLR_final = pd.DataFrame({\n",
    "        'features': X_test.values.tolist(),\n",
    "        'target': y_test,\n",
    "        'prediction': predictionsLR\n",
    "    })\n",
    "    lr_csv_path = \"/tmp/lr_predictions.csv\"\n",
    "    predLR_final.to_csv(lr_csv_path, index=False)\n",
    "    mlflow.log_artifact(lr_csv_path)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e85f8833-9da1-4913-bd41-6787b2adb8aa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Convert Pandas DataFrames to Spark DataFrames\n",
    "predRF_final_spark = spark.createDataFrame(predRF_final)\n",
    "predLR_final_spark = spark.createDataFrame(predLR_final)\n",
    "\n",
    "# Convert array columns to strings (assuming 'features' is the array column)\n",
    "predRF_final_spark = predRF_final_spark.withColumn(\n",
    "    'features', concat_ws(',', col('features'))\n",
    ")\n",
    "predLR_final_spark = predLR_final_spark.withColumn(\n",
    "    'features', concat_ws(',', col('features'))\n",
    ")\n",
    "\n",
    "# Saving predictions to tables\n",
    "predRF_final_spark.write.format('jdbc').options(\n",
    "    url='jdbc:mysql://as7475-gr5069.ccqalx6jsr2n.us-east-1.rds.amazonaws.com/gr5069',\n",
    "    driver='com.mysql.jdbc.Driver',\n",
    "    dbtable='rf_model_predictions',\n",
    "    user='admin',\n",
    "    password='amnashahid99'\n",
    ").mode('overwrite').save()\n",
    "\n",
    "\n",
    "predLR_final_spark.write.format('jdbc').options(\n",
    "    url='jdbc:mysql://as7475-gr5069.ccqalx6jsr2n.us-east-1.rds.amazonaws.com/gr5069',\n",
    "    driver='com.mysql.jdbc.Driver',\n",
    "    dbtable='lr_model_predictions',\n",
    "    user='admin',\n",
    "    password='amnashahid99'\n",
    ").mode('overwrite').save()\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c65363e7-9c8d-4356-b14e-57b072a7341a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Viewing Random Forest predictions\n",
    "spark.read.format(\"jdbc\").option(\"url\", \"jdbc:mysql://as7475-gr5069.ccqalx6jsr2n.us-east-1.rds.amazonaws.com/gr5069\") \\\n",
    "    .option(\"driver\", \"com.mysql.jdbc.Driver\") \\\n",
    "    .option(\"dbtable\", \"rf_model_predictions\") \\\n",
    "    .option(\"user\", \"admin\") \\\n",
    "    .option(\"password\", \"amnashahid99\") \\\n",
    "    .load().display()\n",
    "\n",
    "# Viewing Linear Regression predictions\n",
    "spark.read.format(\"jdbc\").option(\"url\", \"jdbc:mysql://as7475-gr5069.ccqalx6jsr2n.us-east-1.rds.amazonaws.com/gr5069\") \\\n",
    "    .option(\"driver\", \"com.mysql.jdbc.Driver\") \\\n",
    "    .option(\"dbtable\", \"lr_model_predictions\") \\\n",
    "    .option(\"user\", \"admin\") \\\n",
    "    .option(\"password\", \"amnashahid99\") \\\n",
    "    .load().display()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Applied Data-HW#5-Amna Shahid (1)",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
